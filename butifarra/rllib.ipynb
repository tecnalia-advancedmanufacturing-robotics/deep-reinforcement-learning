{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 10:47:18,956\tWARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm as tqdm\n",
    "from butifarra_env import ButifarraEnv\n",
    "import pettingzoo\n",
    "from collections import defaultdict\n",
    "import supersuit\n",
    "import ray.rllib.algorithms.ppo as ppo\n",
    "import stable_baselines3 as stable\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/inigo/.local/lib/python3.8/site-packages/ray/rllib/examples/models/action_mask_model.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inigo/.local/lib/python3.8/site-packages/pettingzoo/utils/wrappers/base.py:58: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "  warnings.warn(\n",
      "/home/inigo/.local/lib/python3.8/site-packages/pettingzoo/utils/wrappers/base.py:72: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x7f9b68e61160>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pettingzoo to stable baselines\n",
    "env = ButifarraEnv(action_mask_on_obs=True, bad_action_reward=0)\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.examples.policy.random_policy import RandomPolicy\n",
    "from ray.rllib.examples.models.shared_weights_model import (\n",
    "    SharedWeightsModel1,\n",
    "    SharedWeightsModel2,\n",
    ")\n",
    "from ray.rllib.examples.models.action_mask_model import TorchActionMaskModel, ActionMaskModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "import ray.rllib.examples.models.action_mask_model\n",
    "print(ray.rllib.examples.models.action_mask_model.__file__)\n",
    "\n",
    "register_env(\"butifarra\", lambda config: PettingZooEnv(env))\n",
    "ModelCatalog.register_custom_model(\n",
    "    \"am_model\",\n",
    "    TorchActionMaskModel\n",
    ")\n",
    "\n",
    "class MyCallback(DefaultCallbacks):\n",
    "    def on_episode_end(self, worker, base_env, policies, episode,\n",
    "                **kwargs):\n",
    "        for (agent_id, agent_policy), agent_rewards in episode.agent_rewards.items():\n",
    "            # raise NotImplementedError(f\"{agent_id=} {agent_rewards=}\")\n",
    "            episode.custom_metrics[agent_id + '_reward'] = agent_rewards\n",
    "\n",
    "\n",
    "policies = {\n",
    "    \"random\": (RandomPolicy, env.observation_spaces[\"player_0\"], env.action_spaces[\"player_0\"], {}),\n",
    "    \"trained\": (None, env.observation_spaces[\"player_1\"], env.action_spaces[\"player_1\"], {}),\n",
    "}\n",
    "\n",
    "\n",
    "def policy_chooser(agent_id, episode, **kwargs):\n",
    "    if agent_id in [\"player_0\", \"player_2\"]:\n",
    "        return \"random\"\n",
    "    else:\n",
    "        return \"trained\"\n",
    "\n",
    "config = ppo.PPOConfig()\n",
    "config.environment(\"butifarra\")\n",
    "config.framework(\"torch\")\n",
    "config.multi_agent(policies=policies, policy_mapping_fn=policy_chooser, policies_to_train=[\"trained\"])\n",
    "config.reporting()\n",
    "config.training(model={\"custom_model\": \"am_model\",\n",
    "                       \"custom_model_config\": {\n",
    "                           \"mask_invalid_actions\": True,\n",
    "                       }})\n",
    "config.training(_enable_learner_api=False)._enable_rl_module_api = False\n",
    "config.callbacks(MyCallback)\n",
    "# config = {\n",
    "#     \"env\": \"butifarra\",\n",
    "#     \"framework\": \"torch\",\n",
    "#     \"model\": {\n",
    "#         \"custom_model\": \"am_model\",\n",
    "#     },\n",
    "#     \"multiagent\": {\n",
    "#         \"policies\": policies,\n",
    "#         \"policy_mapping_fn\": policy_chooser,\n",
    "#         \"policies_to_train\": [\"player_1\", \"player_3\"]\n",
    "#     },\n",
    "#     \"reporting\": {},\n",
    "#     \"training\": {\n",
    "#         \"_enable_learner_api\": False,\n",
    "#     },\n",
    "#     \"rl_modules\": {\n",
    "#         \"_enable_rl_module_api\": False,\n",
    "#     },\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-11-30 11:15:38</td></tr>\n",
       "<tr><td>Running for: </td><td>00:28:16.54        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.8/62.6 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 3.0/24 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_butifarra_7543b_00000</td><td>RUNNING </td><td>172.17.110.30:377084</td><td style=\"text-align: right;\">  1179</td><td style=\"text-align: right;\">         1659.61</td><td style=\"text-align: right;\">4716000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            648.49</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=377084)\u001b[0m 2023-11-30 10:47:25,253\tWARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\n",
      "\u001b[36m(RolloutWorker pid=377150)\u001b[0m /home/inigo/.local/lib/python3.8/site-packages/pettingzoo/utils/wrappers/base.py:58: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[36m(RolloutWorker pid=377150)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(RolloutWorker pid=377150)\u001b[0m /home/inigo/.local/lib/python3.8/site-packages/pettingzoo/utils/wrappers/base.py:72: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[36m(RolloutWorker pid=377150)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(RolloutWorker pid=377150)\u001b[0m 2023-11-30 10:47:28,905\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[36m(RolloutWorker pid=377150)\u001b[0m 2023-11-30 10:47:28,911\tWARNING catalog.py:628 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!\n",
      "\u001b[36m(PPO pid=377084)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[36m(PPO pid=377084)\u001b[0m 2023-11-30 10:47:30,379\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[36m(PPO pid=377084)\u001b[0m 2023-11-30 10:47:30,379\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[36m(PPO pid=377084)\u001b[0m /home/inigo/.local/lib/python3.8/site-packages/numpy/core/_methods.py:181: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[36m(PPO pid=377084)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[36m(RolloutWorker pid=377151)\u001b[0m 2023-11-30 10:47:28,894\tWARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "2023-11-30 11:15:39,323\tINFO tune.py:1047 -- Total run time: 1697.64 seconds (1696.53 seconds for the tuning loop).\n",
      "2023-11-30 11:15:39,325\tWARNING tune.py:1062 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/home/inigo/ray_results/PPO_2023-11-30_10-47-19\", trainable=...)\n"
     ]
    }
   ],
   "source": [
    "from ray import air, tune\n",
    "results = tune.Tuner(\n",
    "    \"PPO\", param_space=config, run_config=air.RunConfig(stop={\"training_iteration\": 10000}, verbose=1)\n",
    ").fit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
