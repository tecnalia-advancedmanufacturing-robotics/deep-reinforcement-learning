{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnityTimeOutException",
     "evalue": "The Unity environment took too long to respond. Make sure that :\n\t The environment does not need user interaction to launch\n\t The Academy and the External Brain(s) are attached to objects in the Scene\n\t The environment and the Python interface have compatible versions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnityTimeOutException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6404506da129>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnityEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"p1_navigation/Banana_Linux/Banana.x86_64\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_name, worker_id, base_port, curriculum, seed, docker_training, no_graphics)\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0maca_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_academy_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_init_parameters_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnityTimeOutException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36msend_academy_parameters\u001b[0;34m(self, init_parameters)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnityInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_initialization_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_initialization_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrap_unity_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrl_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnityRLInput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnityOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             raise UnityTimeOutException(\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0;34m\"The Unity environment took too long to respond. Make sure that :\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0;34m\"\\t The environment does not need user interaction to launch\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;34m\"\\t The Academy and the External Brain(s) are attached to objects in the Scene\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnityTimeOutException\u001b[0m: The Unity environment took too long to respond. Make sure that :\n\t The environment does not need user interaction to launch\n\t The Academy and the External Brain(s) are attached to objects in the Scene\n\t The environment and the Python interface have compatible versions."
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"p1_navigation/Banana_Linux_NoVis/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from collections import deque\n",
    "\n",
    "def train(agent, n_episodes, max_steps, eps_start, eps_end, eps_decay, success_reward):\n",
    "    \n",
    "    # Reset scores\n",
    "    scores = []                       \n",
    "    scores_window = deque(maxlen=100) \n",
    "\n",
    "    # Initialize epsilon\n",
    "    eps = eps_start\n",
    "\n",
    "    # Training loop                \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        # Reset environment and get first   \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        \n",
    "        # Get initial state\n",
    "        state = env_info.vector_observations[0]\n",
    "    \n",
    "        # Loop over step by step \n",
    "        score = 0\n",
    "        for t in range(max_steps):\n",
    "\n",
    "            # Epsilon-greedy action selection\n",
    "            action = agent.act(state, eps)\n",
    "\n",
    "            # Take action\n",
    "            env_info = env.step(int(action))[brain_name]\n",
    "\n",
    "            # Get next state\n",
    "            next_state = env_info.vector_observations[0]   \n",
    "\n",
    "            # Get reward\n",
    "            reward = env_info.rewards[0]\n",
    "\n",
    "            # Get done              \n",
    "            done = env_info.local_done[0]  \n",
    "\n",
    "            # Take one step (add experience & learn)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        scores_window.append(score)       \n",
    "        scores.append(score)              \n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        \n",
    "        # If the we reached the minimum success reward -> Environment solved, save model\n",
    "        if np.mean(scores_window)>= success_reward:\n",
    "            print('\\nEnvironment solved in {:d} episodes!')\n",
    "            print('Average Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent \n",
    "GAMMA = 0.99            # Discount factor\n",
    "TAU = 1e-3              # For soft update of target parameters\n",
    "LR = 5e-4               # Learning rate\n",
    "UPDATE_EVERY = 8        # How often to update the network\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # Replay buffer size\n",
    "BATCH_SIZE = 128         # Minibatch size\n",
    "\n",
    "from dqn_agent import Agent\n",
    "agent = Agent(state_size, action_size, GAMMA, TAU, LR, UPDATE_EVERY, BUFFER_SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "N_EPISODES = 2000\n",
    "MAX_STEPS = 2000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 0.99\n",
    "\n",
    "SUCCESS_REWARD = 14\n",
    "\n",
    "scores = train(agent, N_EPISODES, MAX_STEPS, EPSILON_START, EPSILON_END, EPSILON_DECAY, SUCCESS_REWARD)\n",
    "\n",
    "# Plot the scores\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "# plt.savefig('training_best.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self, env, brain_name,filename):\n",
    "    weights = torch.load(filename)\n",
    "    self.qnetwork_local.load_state_dict(weights)\n",
    "    self.qnetwork_target.load_state_dict(weights)\n",
    "        \n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "\n",
    "    for t in range(2000):\n",
    "        action = self.act(state, 0)\n",
    "        env_info = env.step(action.astype(np.int32))[brain_name] \n",
    "        next_state = env_info.vector_observations[0]         \n",
    "        done = env_info.local_done[0]      \n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create environment (this time with visualization)\n",
    "env = UnityEnvironment(file_name=\"p1_navigation/Banana_Linux/Banana.x86_64\")\n",
    "\n",
    "# Get brain name\n",
    "brain_name = env.brain_names[0]\n",
    "\n",
    "test(env, brain_name, \"checkpoint.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset the environment for training\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# Number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# Examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "\n",
    "# Create the agent\n",
    "agente_bananero = Agent(state_size, action_size)\n",
    "\n",
    "# Train the agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b5df25e339b3354111e4866bba355b89ce2b59abf0ac9682082557eed468901d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
