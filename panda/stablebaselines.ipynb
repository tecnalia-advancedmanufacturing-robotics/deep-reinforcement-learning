{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 17:42:29.627711: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-30 17:42:29.662761: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-30 17:42:30.178381: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "pybullet build time: Nov 28 2023 23:51:11\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import stable_baselines3\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "import sb3_contrib\n",
    "import panda_gym.envs\n",
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('achieved_goal': Box(-10.0, 10.0, (3,), float32), 'desired_goal': Box(-10.0, 10.0, (3,), float32), 'observation': Box(-10.0, 10.0, (6,), float32))argv[0]=--background_color_red=0.8745098114013672\n",
      "argv[1]=--background_color_green=0.21176470816135406\n",
      "argv[2]=--background_color_blue=0.1764705926179886\n",
      "\n",
      "Box(-1.0, 1.0, (7,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = panda_gym.envs.PandaReachEnv(control_type=\"joints\")\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=--background_color_red=0.8745098114013672\n",
      "argv[1]=--background_color_green=0.21176470816135406\n",
      "argv[2]=--background_color_blue=0.1764705926179886\n"
     ]
    }
   ],
   "source": [
    "class ActionPenalizerWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        reward -= np.mean(abs(action)) * 0.01\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "def getenv():\n",
    "    env = panda_gym.envs.PandaPushEnv(control_type=\"joints\", reward_type=\"dense\", render_mode=\"rgb_array\")\n",
    "    env.task.distance_threshold = -1\n",
    "    # flatten wrapper\n",
    "    env = gym.wrappers.FlattenObservation(env)\n",
    "    env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n",
    "    env = gym.wrappers.TransformReward(env, lambda r: -r**2)\n",
    "    env = ActionPenalizerWrapper(env)\n",
    "    # env = gym.wrappers.RecordVideo(env, \"./runs/RecurrentPPO\", lambda ep: ep % 100 == 0)\n",
    "    return env\n",
    "\n",
    "n_envs = 1\n",
    "vec_env = stable_baselines3.common.env_util.make_vec_env(getenv, n_envs=n_envs)\n",
    "# vec_env = stable_baselines3.common.vec_env.VecVideoRecorder(vec_env, \"./runs/RecurrentPPO\", lambda ep: ep % 10 == 0, video_length=100)\n",
    "\n",
    "if True:\n",
    "    model = sb3_contrib.RecurrentPPO(\n",
    "        'MlpLstmPolicy',\n",
    "        vec_env,\n",
    "        verbose=0,\n",
    "        tensorboard_log=f\"./runs/RecurrentPPO/\",\n",
    "        policy_kwargs=dict(net_arch=[128, 64]),\n",
    "        n_steps= 2048//n_envs,\n",
    "        gamma=0.99,\n",
    "        ent_coef=.01\n",
    "\n",
    "    )\n",
    "    stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)\n",
    "    eval_callback = EvalCallback(vec_env, eval_freq=204800//n_envs, callback_after_eval=stop_train_callback, verbose=1)\n",
    "    model.learn(total_timesteps=1000000, tb_log_name=\"versus random\", progress_bar=False, log_interval=1, callback=eval_callback)\n",
    "    model.save(f\"./runs/RecurrentPPO2\")\n",
    "else:\n",
    "    model = sb3_contrib.RecurrentPPO.load(f\"./runs/RecurrentPPO.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model\n",
    "\n",
    "env = gym.wrappers.RecordVideo(getenv(), \"./runs/RecurrentPPO\")\n",
    "vec_env = stable_baselines3.common.env_util.make_vec_env(lambda: env)\n",
    "\n",
    "obs = vec_env.reset()\n",
    "# cell and hidden state of the LSTM\n",
    "lstm_states = None\n",
    "num_envs = 1\n",
    "# Episode start signals are used to reset the lstm states\n",
    "episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "score = 0\n",
    "while True:\n",
    "    action, lstm_states = model.predict(obs, state=lstm_states, episode_start=episode_starts, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    score+=rewards.mean()\n",
    "    episode_starts = dones\n",
    "    if dones.all():\n",
    "        break\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
