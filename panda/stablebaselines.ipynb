{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import stable_baselines3\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement, CallbackList, CheckpointCallback\n",
    "import sb3_contrib\n",
    "import panda_gym.envs\n",
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = panda_gym.envs.PandaReachEnv(control_type=\"joints\")\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionPenalizerWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        reward -= np.mean(abs(action)) * 0.01\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "def getenv():\n",
    "    env = panda_gym.envs.PandaPushEnv(control_type=\"joints\", reward_type=\"dense\", render_mode=\"rgb_array\")\n",
    "    env.task.distance_threshold = -1\n",
    "    # flatten wrapper\n",
    "    env = gym.wrappers.FlattenObservation(env)\n",
    "    env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n",
    "    # env = gym.wrappers.TransformReward(env, lambda r: -r**2)\n",
    "    env = ActionPenalizerWrapper(env)\n",
    "    # env = gym.wrappers.RecordVideo(env, \"./runs/RecurrentPPO\", lambda ep: ep % 100 == 0)\n",
    "    return env\n",
    "\n",
    "n_envs = 1\n",
    "vec_env = stable_baselines3.common.env_util.make_vec_env(getenv, n_envs=n_envs)\n",
    "# vec_env = stable_baselines3.common.vec_env.VecVideoRecorder(vec_env, \"./runs/RecurrentPPO\", lambda ep: ep % 10 == 0, video_length=100)\n",
    "\n",
    "if True:\n",
    "    model = sb3_contrib.RecurrentPPO(\n",
    "        'MlpLstmPolicy',\n",
    "        vec_env,\n",
    "        verbose=0,\n",
    "        tensorboard_log=f\"./runs/RecurrentPPO/\",\n",
    "        policy_kwargs=dict(net_arch=[128, 64]),\n",
    "        n_steps= 2048//n_envs,\n",
    "        gamma=0.99,\n",
    "        ent_coef=.01\n",
    "    )\n",
    "    stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=2, min_evals=5, verbose=1)\n",
    "    eval_callback = EvalCallback(vec_env, eval_freq=20000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "    checkpoint_callback = CheckpointCallback(save_freq=20000, save_path=\"./runs/RecurrentPPO\", name_prefix=\"RecurrentPPO\")\n",
    "    callback = CallbackList([eval_callback, checkpoint_callback])\n",
    "    model.learn(total_timesteps=1000000, tb_log_name=\"versus random\", progress_bar=False, log_interval=1, callback=callback)\n",
    "    model.save(f\"./runs/RecurrentPPO2\")\n",
    "else:\n",
    "    model = sb3_contrib.RecurrentPPO.load(f\"./runs/RecurrentPPO.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model\n",
    "\n",
    "env = gym.wrappers.RecordVideo(getenv(), \"./runs/RecurrentPPO\")\n",
    "vec_env = stable_baselines3.common.env_util.make_vec_env(lambda: env)\n",
    "\n",
    "obs = vec_env.reset()\n",
    "# cell and hidden state of the LSTM\n",
    "lstm_states = None\n",
    "num_envs = 1\n",
    "# Episode start signals are used to reset the lstm states\n",
    "episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "score = 0\n",
    "while True:\n",
    "    action, lstm_states = model.predict(obs, state=lstm_states, episode_start=episode_starts, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    score+=rewards.mean()\n",
    "    episode_starts = dones\n",
    "    if dones.all():\n",
    "        break\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
